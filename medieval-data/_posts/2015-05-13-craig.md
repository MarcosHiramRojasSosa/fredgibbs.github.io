--- 
layout: post 
title: The tragedy of corralled data and open projects
date: 2015-05-13 10:00:00
---

**Kalani Craig**

It's something of a tragedy that we have a whole world of archival
material out there, digitized, transcribed and largely unavailable for
analysis because it's behind massive paywalls. The phrase "open data"
often conjures up visions of herds of information roaming free across
the digital plains waiting to be corralled by avid scholars, but the
reality of the data landscape for medievalists is much more complicated.
The digitized and transcribed texts available to medievalists are
largely fenced-in digitized sources—notably the MGH and the PL—with a
few roaming hand-transcribed sources that constitute fairly small
collections, comparatively. The prevalence of these copyrighted
digitized texts, many with very restrictive copyright and usage
guidelines that limit data scraping, can be a limiting factor in
applying text mining, hGIS and network-theoretical approaches.<span
id="fnlink1"></span>[<sup>[1]</sup>](#fn1)

The biggest issue with using these big-data techniques on closed,
paywalled data is that we lose the citations that make verifiability and
repeatability possible. These process-oriented buzzwords are often used
to describe STEM projects, but for digital humanities scholars,
repeatability and verifiability mean giving other people the chance to
redo your work to support what you've done, to prove you're wrong, to
add nuance to the things that you've done. Repeatability for humanities
scholars means not the ability to confirm that someone else did
something right but to examine their methodology and its underlying
theoretical implications and change them as the underlying theoretical
viewpoints also change.

As a compromise between open-data ideals and closed-data sources, I'm
looking at workflow options for individual scholars analyzing full text
from restricted sources, starting with medieval Latin but with the idea
that the process itself can be abstracted and used regardless of the
language in question. This full-text analysis process requires
integrating SQL, automated scripting and linguistic tagging tools like
NLTK and TnT in order to preserve the citations and context of the
original protected source. This preservation of citation data—which is
often lost or obscured as we work with topic modeling and
natural-language processing software—is geared toward open data's most
important ideal: giving other scholars, digital and nondigital alike,
access to, and the ability to reproduce, big-data approaches while still
respecting the usage guidelines that our field's most prolific
digitizers require.

### Corpus Linguistics and Network Analysis

Two technologies common to digital historians combine to make the
underlying necessities in this process more visible: corpus linguistics,
a type of text-mining approach that documents the relationship between
all the words in a corpus of texts; and network analysis, which looks at
the patterns in connections between different elements of a system. In
this case, corpus linguistics uncovers the relationships that develop as
an author uses a specific set of words to define a single phenomenon.
Those relationships can then be mapped and more easily viewed using
network analysis.

For the workflow, the value in combining these sources is in how the
data itself is manipulated. Corpus linguistics diverges from other text
mining processes but it is still at its core dependent on an analysis of
common word pairings.<span id="fnlink2"></span>[<sup>[2]</sup>](#fn2)
Where corpus linguistics, differs, however, is in its focus on using
every single word in a corpus as a part of the analysis rather than
limiting its analytical scope to the top few thousand most frequent
words. This focus emphasizes the citation value of each word.
Additionally, corpus linguistics privileges the "tokenizing" process, in
which each word and punctuation mark in a text is transformed into a
single element in a larger whole. However, it then takes those elements
and looks at collocation, co-occurrence and other measures that abstract
the word itself from its placement in the text. That can make it hard to
trace the collocates back to their original sources in a very large
corpus, and thus corpus linguistics forces us to focus on the very small
without losing sight of how those tiny items aggregate into a whole.

Similarly, network analysis is focused on every single item in a system.
We can use the corpus-linguistics-produced tokenized data for
visualization, and focus on the analysis of each token, but only by
further distancing it from the original paywalled source—and its
citations and search ability—by way of taking collocates or
co-occurrences and aggregating them visually. Accommodating network
analysis and corpus linguistics in the workflow forces the accommodation
of other forms of visualization that value the individual occurrence of
tokens, like GIS, and other forms of data mining, like named entity
recognition, while still maintaining the relationship of lemmatized or
tagged tokens with their unlemmatized counterparts and without
completely divorcing the lemma from the citations in the original.

### The process *in situ*

It helps to take a brief side trip into the project on which this
workflow is based. When eleventh- and twelfth-century reformers talk
about medieval church reform, they use particular linguistic patterns.
For instance, they use the Latin *corrigere* to describe correction for
monks and clerics who had strayed from canonical behaviors. Similarly,
they use *purgare* to describe the removal of monks and canons who were
bad influences from a local monastic center or a cathedral chapter. But
how pervasive are these particular vocabulary choices and what
historical phenomenon do they most commonly describe? What if we could,
to borrow a term from the world of hi-tech, surf through the change and
continuity that surrounds a historical problem by following words that
share similar usage patterns? That would involve finding word pairs that
are used similarly within the scope of a single text, and then comparing
those usage patterns to another corpus of texts. For instance, we might
begin with *corrigere*, and the nouns and adjectives and adverbs that
affect and are affected by *corrigere*, in a single text, in order to
find other verbs with similar associations.

Repeating this process for other verbs, and then for other texts,
produces a thesaurus of sorts. However, instead of being static and
composed of all of the synonyms that have ever been used for the words
in which we are interested, the result is several dynamic thesauruses
based on a series of texts that we define and that can then be compared
against other similarly constructed thesauruses. This hyperthesaurus, if
we're using the linked cities that make up a single urban environment in
HyperCities as a model for thinking about the linking of concepts that
change over time,<span id="fnlink3"></span>[<sup>[3]</sup>](#fn3)
compares patterns at the individual text level with patterns in groups
of texts and across the corpus to uncover the linguistic patterns that
are used to describe a particular historical phenomenon.

Using a custom database external to the paywalled sources from which we
draw our work is necessary to accommodate not just the multiple
paywalled sources with which we work but also a variety of critical
editing styles in Latin texts. It makes orthographic standardization
("v" to "u") much easier, but more importantly, it allows us to
integrate custom and off-the-shelf tools that handle tokenization, or
division of the text into individual words, and tree banking (which
applies sentence diagramming and part-of-speech tagging to recreate the
grammar of a sentence).

There are a host of manually-tagged corpora available for classical
Latin (Tufts' Perseus corpus chief among them) but most of the medieval
Latin corpora like the PL, MGH, and CLCLT are merely lemmatized rather
than treebanked. Perseus, on the other hand, contains manual
part-of-speech tagging and treebanking, and as such, it has some key
elements that guide the data import process for medievalists looking to
manage their own big-data analyses projects. Chief among these is a set
of localized citation information—column numbers for the PL, for
instance—to sentence and word order during the import into an SQL
database. Doing so is vital to backtracking at the end of the analytical
process.

{% include figure.html class="raw" src="/images/medieval-data/Diagram1.png" caption="Note the excess of data like book, chapter, and line number in addition to column and page, which are all designed to allow for granular citation at the end of a project without requiring the complete export of a text to the audience of a scholarly project." %}


A custom tagger marks elements of the text that can interfere with a
corpus-linguistics analysis—proper names, roman numerals, digits—<span
id="fnlink4"></span>[<sup>[4]</sup>](#fn4)and then the text is tagged
for comparison against the dictionary generated by Latin Words. Finally,
the text is compared to the hand-tagged corpus from Tufts' Perseus
project in both TreeTagger and TnT, two natural language processors that
can lemmatize, or stem, each token.<span
id="fnlink5"></span>[<sup>[5]</sup>](#fn5) Unknown words from TT and TnT
are compared to possible roots in the Latin Words scan of the text
manually. The exports to external tools (network analysis, collocation,
etc.) use a concatenation of all three tags—dictionary-based, TT and
TnT—in order to provide a more clear view of word usage in the corpus.

These results are representative of a basic corpus linguistics analysis,
which includes root words and parts of speech for each word in our
corpus, but it's not a simple tool. This database is flexible. The value
here is in the yoking of several corpus-linguistics tools to a single
original citation. Not only are results from TreeTagger and TnT now
available side by side, it's possible to export some of these
collocations manually into tools like Gephi and still have a search
key—here, again, the root word column—that integrates the paywalled
citations for each occurrence regardless of its source. Text exports for
collocation analysis can now be done on a root-word basis, as can
searches on the root-word column to find occurrences of incipio across a
corpus of my choosing, rather than across a corpora like the PL or MGH
that has limits on the ways texts can be combined, searched, and
exported.

Let's look at one of the resulting networks and the citation output that
could accompany it as a way of providing search tools that . Here's a
filtered set of verbs from Sulpicius Severus' Life of St Martin (PL,
Vol. 20, Col.0159A-0176C). *Ecclesia*, the church, appears in purple in
the lower-left corner of this network as a one element of a series of
gentle efforts to bring the church into the hearts and minds of pagan
Frankia. Belief in God (*credo* and *dominus* together*)* and conversion
(*converro*) play a significant role, but *civitas*, *domus* and *locus*
are also closely connected, which suggests a careful rooting of the
church in both the spiritual and the physical.

{% include figure.html class="raw" src="/images/medieval-data/EcclesiaSource5.png" caption="" %}

Accompanying that is a citation search from the database. This is one of
several hundred that would appear—easily exportable because of the
filtered output from this network analysis—with this diagram, but it
gives you a good idea of how easy it would be to double check this
diagram against the original paywalled source.

{% include figure.html class="raw" src="/images/medieval-data/Citations.png" caption="Citations of Ecclesia in St. Martin in the Patrologia" %}


### The process abstracted

My goals here are broad applicability—an abstraction of the process not
just beyond my data but beyond medieval Latin—and open distribution
(though perhaps open source is too specific a word to use at this point,
as is open access). There are other scholars out there who don't have
the expertise to work through the necessary steps with all of these
tools, but whose expertise would allow them to follow a set of
documented steps and use existing training corpora.

Additionally, because Latin is informal in its composition, and because
there are few manually tagged corpora, it offers a study in how to
manage corpora outside of the English-corpus norm. Many of the tools
that function so well for English are less effective even when they're
trained on an immediately applicable Latin corpus. Similarly, the
automated training for medieval Latin in this process is based on
manually tagged corpora of classical Latin, so this process is also more
valuable for scholars whose primary working language is dependent on
automated corpus processing with limited, and imperfect, available
trained corpora.

Because these questions reach far beyond the borders of digital history,
the computational linguists at Indiana University have signed on to help
formalize the process. While my tools are kludgy and not ready for prime
time, the process itself and is at a stage where its possible to start
formalizing the documentation. That means we'll start working through
the process with other scholars, other corpora and other languages and
releasing documentation and tools to support it.

We've got several pieces of this documentation puzzle in place. The
first is an intern to work through some of the manual linguistics
manipulation that needs to be done when your trained corpora are as
loosely related as, say, classical Latin and medieval Latin. Comparing,
and correcting, the lacunae in the lemmatization results of the three
main NLP tools is a massive undertaking, but here again, the process and
its focus on corpus linguistics helps limit the manual work and provide
a training corpus.

However, that still doesn't manage the twin questions of repeatability
and verifiability in their entirety. Copy and paste will still be part
of the system because the underlying data itself can't be distributed.
Each scholar will have to run their own lemmatizing process, build their
own corpus from an existing paywalled corpus. In an ideal world, our
paywalled sources will do one of two things: they'll make use of this
kind of work and develop the tools we need to function as digital
humanists as part of their data, or they'll be less restrictive about
how their data is used and make it easier for us to do this kind of work
on their data.

In the long run, though, the big question is whether we as scholars will
have to keep investing in this kind of time. The chasm between paywalled
data and digital-humanities project will always be time, because it's
time that dictates repeatability. My hope is that this process, which
will have mistakes in it, will at least have consistent and documented
mistakes that can save other scholars the time investment that I pointed
out early on in this paper.

------------------------------------------------------------------------

<span id="fn1"></span>[<sup>[1]</sup>](#fnlink1)Thanks to last year's
EPISCOPUS panel (Maureen Miller, Steven Vanderputten, Jehangir Yezdi
Malegam, and Julia Barrow) and to Sandra Kübler, Scott Weingart, and
Michelle Moravec for their technical and methodological expertise.

I also need to acknowledge that this work was supported in part by
Shared University Research grants from IBM, Inc., to Indiana University
and based on work supported by the National Science Foundation under
Grant No. CNS-0723054. Any opinions, findings and conclusions, or
recommendations expressed in this material are those of the author(s),
and do not necessarily reflect the views of the National Science
Foundation (NSF).

<span id="fn2"></span>[<sup>[2]</sup>](#fnlink2)Where corpus linguistics
differs from topic modeling is in its granularity. Topic modeling
ignores common stop words like "and" or "the", or even "woman" depending
on its popularity, in order to find the shifts in vocabulary clusters
that characterize large chunks of text. A corpus-linguistics approach,
which includes natural-language processing, provides part-of-speech
information and pays attention to the stop-words that topic modeling
does not. See Michelle Moravec,
http://historyinthecity.blogspot.com/2013/12/corpus-linguistics-for-historians.html,
and S. Wallis and G. Nelson'Knowledge discovery in grammatically
analysed corpora' (*Data Mining and Knowledge Discovery* 5, 2001:
307–340).

<span id="fn3"></span>[<sup>[3]</sup>](#fnlink3)Todd Presner, David
Shepard, and Yoh Kawano, *HyperCities: Thick Mapping in the Digital
Humanities* (Harvard University Press, 2014)

<span id="fn4"></span>[<sup>[4]</sup>](#fnlink4)Though I'm not using
Named Entity Recognition, because there are few tags for this, although
Recogito has begun to do some of this at
http://pelagios-project.blogspot.com/2014/06/what-have-romans-ever-mapped-for-us.html.

<span id="fn5"></span>[<sup>[5]</sup>](#fnlink5)Studies of corpus
linguistics specifically for Latin--using TreeTagger in particular--see
Arne Skjærholt, "More, Faster: Accelerated Corpus Annotation with
Statistical Taggers." *Journal for Language Technology and Computational
Linguistics,* Vol. 26, No. 2. (2011), pp. 151-163 26, no. 2 (2011):
151-163. A more detailed look at the value of judiciously planned
interaction between researchers curating data and automatic tagging of
data is in Gregory Garretson and Mary Catherine O'Connor, "Between the
Humanist and the Modernist: Semi-Automated Analysis of Linguistic
Corpora" (in *Corpus Linguistics Beyond the Word: Corpus Research from
Phrase to Discourse,* edited by Eileen Fitzpatrick; Amsterdam: Rodopi,
2007), 87-106.
