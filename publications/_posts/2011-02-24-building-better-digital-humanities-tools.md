--- 
layout: post 
title: Building Better Digital Humanities Tools
date: 2011-02-24 00:00:00
---

# Building Better Digital Humanities Tools:  Toward broader audiences and user-centered designs

Fred Gibbs, George Mason University 
Trevor Owens, Library of Congress

#Introduction
Despite significant investments in digital humanities tool development, most tools have remained a fringe element in humanities scholarship. As the 2005 Summit on Digital Tools at the University of Virginia found, "only about six percent of humanist scholars go beyond general purpose information technology and use digital resources and more complex digital tools in their scholarship" (Summit, 2006). Although this percentage is probably higher now, it is still far lower than most digital humanists would prefer. Low adoption has led to some questions about how much adoption is a priority for tool builders. One study, for example, has shown significant concerns about the extent to which the developers of digital tools make an effort to have their tools used by practicing scholars (Bradley, 2008). More recently, another study indicated that only about half of tool developers considered the number of users that adopted a tool as an indicator of a success. Only about a third ran usability studies, and a disappointing 14% conducted surveys about the tool (Schreibman and Hanlon, 2010). Lack of adoption has remained a difficult problem to solve, partly because it remains unclear how scholars expect tools to behave, how they want to interact with different kinds of tools, and how they perceive their present and future utility.

Recent funding from the National Endowment for the Humanities has allowed the Center for History and New Media to extend and formalize some of its ongoing dialog with users about the general usability of a handful of research tools for digital history, especially for text-mining and visualization--two of the more active areas of inquiry and tool development in recent years. Through an open ended survey and a virtual panel discussion, our interested but skeptical audience provided their take on the usability of digital tools and resources targeted at historians.

The results of the survey and panel discussion reveal the variety of users interested in digital tools, their expectations, and typical confusion that has created barriers to tool use and to wider adoption of new research methodologies. Although our survey and discussion are neither comprehensive nor definitive, we hope that the comments and attitudes of the users will help tool builders produce more effective and more widely used tools for humanities research.

##Survey of Existing Practices 
We first developed a survey that allowed a diverse group of 213 historians to voice their interests, concerns, and views about digital research methodologies. Of these, roughly half were graduate students, and roughly half were professors at various stages of their careers. In many cases, these historians shared roughly 500-word reactions and thoughts about their experiences using digital tools in their research. We published the survey openly online, and invited participation through the popular History News Network and through direct solicitations of graduate history departments. Our aim was to provide a space for historians to present their own ideas, not simply an opportunity to agree or disagree with our ideas. To this end we asked our participants to answer a set of six open-ended questions about how they used existing tools and resources, and the extent to which these tools and resources met their needs. Below we summarize the most common sentiments relating to how historians viewed their own tool use.

*Historians use technology to speed up traditional research methodologies.*  When asked to “give examples of how digital resources and tools are allowing you to do research (and to learn things) that you couldn’t have done ten or twenty years ago” historians gave two primary responses. First, they focused on ease of access and the way in which various databases allow them to more quickly access things they would have otherwise walked to the library for. As one user reported, "I start with Google to get a sense of what might be out on the web and then I go to specific resources that I know of such as the IMB, WorldCat, etc." Second, they reported that they use Google and Google Books to track down unusual terms and unique quotations. Much like Patrick Leary described in his 2005 article, "Googling the Victorians," these historians use Google search and Google Books as a means to find references to obscure people and events. One typical response: "If I type a person's name or event (especially obscure ones) into Google books, I can find works outside of my major field that make reference to them."

*Historians make extensive use of Google and JSTOR for initial queries, expecting resources to be visible with high level searches.*  When asked “which online or digital resources have you used in your scholarly research and writing”, almost all resources mentioned are repositories of either primary or secondary source material. Google services were mentioned a total of 100 times by 70 participants, and JSTOR was mentioned 99 times by 98 participants. 72 participants made reference to "library" or "libraries" sites, 20 of which were references to the Library of Congress Online Catalog. For comparison: 25 participants mentioned ProQuest, 23 mentioned ARTstor, and 19 mentioned Wikipedia. One of the key reasons individuals gave for using these specific sites was their ease of use; individuals reported feeling lost on many other sites because there wasn't an intuitive interface. Users reported that they were generally unwilling to dig deep into sites to find information. In short, the lesson here is that information needs to be visible with high-level searches.

*Historians tend to prefer quantity to quality with digitized primary and secondary sources.*  Historians expressed overwhelming gratitude for the availability of online primary and secondary source material. In contrast to other disciplines like philology or textual criticism, where exact transcription is crucial, historians frequently preferred resources that offer large quantities of materials with even a crude full-text component. This sentiment likely reflects their primary use of technology, namely that finding references and information is a much higher priority than using tools to analyze primary sources. At the same time, the respondents were concerned about gated access to resources and the quality of the resources that are freely available online. Respondents referred to the adage "you get what you pay for" several times in reference to the quality of non-copyrighted materials that are often used to create full text resources. This is in reference to the fact that many easily available texts are late nineteenth- or early twentieth-century editions/translations that have been obviated by more recent scholarship. But because these older ones are now in the public domain, they are able to be scanned and ingested into large repositories like the Internet Archive.

*Historians show little knowledge of digital tools to aid in historical analysis.*  While historians were excited about repositories of primary source materials online, they made little comment about a need for, or interest in, any specific tools to help make use of these archives in novel ways. With the exception of a few mentions of Zotero (11) and Endnote (5), there was no mention of third-party tools, or methodologies involving text/data analysis and visualization. Nor was there significant mention of any emergent technologies, like projects and standards that leverage the semantic web for information discovery. This might suggest that there is little general interest in such tools and technologies, but as our panel discussion (as described below) suggests, the average historian might be considerably more interested in using digital tools for research than has been recognized. 
Overall, the results confirm what one may have expected: the uses of 'digital tools' among historians are the most general: Google searches and the use of digitized primary and secondary sources. On one level, then, historians use tools for little more than speeding up their existing practices. However, the emerging practice of using Google searches to find obscure terms in obscure texts from other disciplines suggests that even basic search technology is expanding, even if only slightly, standard historical methodologies. As these historians try out searches for terms in the corpus of books Google has digitized they are integrating this digital corpus into the hermeneutic process of testing ideas and theorizing about the past. 

##Panel Discussion 
 To explore more deeply scholars' perspectives on the potential value of specific tools, we convened a smaller panel from the survey participants. Over the course of four months the panel discussed seven different specific digital tools at nexthistory.org, generating 130 comments about the utility and usability of the tools. Though not an especially large sample size, the comments were remarkably consistent and revealing. We think the concerns they express are relevant for most any tool in the digital humanities, especially history, and nicely complement some previous surveys, such as a CLIR report on digital tools, which employed a much larger definition of 'tool' and focused on accessbility (Shilton, 2009).

We chose to solicit comments on tools that we thought together represented a wide range of design, functionality, sophistication, and that showed promise of applicability to variety of fields within history and across other disciplines. Each of these tools has indubitably advanced both practice and theory in the digital humanities, and any criticism of individual tools and projects (both here and in the panel discussion) should not detract from the important contributions these tools have and will continue to make to the field. Without such tools already in place, it would be impossible to have a practical discussion of ways to make tools even better and more widely used. Nor is this to say that these tools have been judged more important than others that were not discussed, though we do not believe choosing more or other tools would have solicited substantially different feedback. 
A number of the tools focused on visualization. Many respondents mentioned that the ways in which they were prompted to think about visualizing their data was quite useful to them in ways in which they did not expect. "Thinking through what I might do and how I might present it," wrote one respondent, "...has helped to sharpen my analysis of my research and find areas where I need to or would like to know more". Another panelist, thinking even more broadly, "...could imagine a visualization that might alter some of the standard narratives." On the other hand, panelists also revealed how striking visualizations are not necessarily inspiring to those who would prefer to continue thinking in terms of texts. Regarding the Favoured Traces visualization that shows how On the Origin of Species changed from one edition to the next, one commenter lamented a distancing from the real text: "I guess I do not see how it would work. The information is shown as flashing lights, rather than the actual text and I did not see anything that looked like page numbers." Such a sentiment serves as a reminder that users may expect to see traditional devices like page numbers even with online texts or visualizations for which they are not entirely appropriate. Interface designs conscious of such users will help more 'traditional' historians feel more comfortable with new ways of visualizing, analyzing, and thinking about sources.

As some of the above quotations suggest, the tools that had the most positive feedback were the ones that easily allowed historians to develop quick views of their data from multiple perspectives--especially if they could help historians navigate the overwhelming amounts of material that they had accumulated. Users wanted to explore and play with ideas quickly. In fact, they commented on how they much preferred 'easy access' tools to those that created (or tried to create) a polished visualization. One of the more popular, if simple, examples was Wordle, a tool that helps visualize word frequency within a given text with a stylish word cloud. One user commented that the ease of use made him want to explore more about how he could use linguistic analysis in his historical research: "If I had done this at the beginning of my research of these texts, it might have inspired me to take my analysis in a different direction." Especially for their ability to generate quick views of data, respondents were enthused about the teaching potential of the tools for giving quick impressions of texts in the classroom. This sentiment signals that the classroom may be an excellent place to encourage use of some of the more straightforward tools, even if in a limited capacity. With familiarity and comfort, a user might well begin to push the tool a bit harder for their own research purposes.

Beyond the benefits of quick visualization, however, users found more to dislike than to like about the tools overall. This appears less a result of the tools per se than of the substantial gap between what the user expected the tool to do (though this generally came from the tool documentation—more on this later) and what the user could get it to do. Comments suggest that our non-technical users either could not generally appreciate what several of the more complex tools were designed to do, or were unable to recognize their potential value in historical research. They complained that much of the documentation was written in a highly technical fashion, and was virtually unintelligible to them. Many panelists struggled with SEASR (=The Software Environment for the Advancement of Scholarly Research), which provides a range of text analysis tools in a virtual environment. Although the website has some helpful videos and other well-written descriptions, the overall documentation was considered far too technical. One panelist remarked that using the site "felt as if I was testing for a foreign language test without enough study." In many cases, individuals had little ability to imagine what any of the individual tools might be truly useful for. One of the most commonly expressed sentiments was a variation of the statement expressed most succinctly by one panelist: "I think it will be useful, but I’m not sure how yet."

In order to help bridge the expectation gap, one panelist cogently suggested that tool builders ought to think about their work as establishing a social contract between them and the user. For a user to even consider using a tool, the tool's website needs to establish that the time devoted to deploying it will generate results that warrant the investment. One user wrote, "So I think that the continued creation of these tools is really important, and think that they need to be explained, with some real-world examples, so that we can have a better sense whether the tool can do what we want." In short, users wanted the theoretical benefits spelled out in plain language irrespective of discipline. Panelists both implicitly and explicitly suggested that tool builders might pitch their tools in slightly different ways. They might, for example, focus more on the immediate research convenience that the tool provides. As one user commented, "I am skeptical that it would reveal 'hidden information' – but if it convinced me that I could save time – and that it was reliable and worked across different languages – then I’d be all ears." Perhaps this is to say that the language used to educate users about the tool needs to be different than the language used to secure funding for a project. Grant language tends to emphasize innovation and revolutionary benefits, rather than more modest quotidian uses, although these are what new users are more likely to find immediately useful. Although less impressive-sounding, a more realistic description of the benefits and limitations could encourage wider adoption of tools, especially considering that most users will not be prepared to make sophisticated and revolutionary use of the tool right away.

Perhaps growing out of the confusion about the possible research utility of a given tool, another common complaint was that the tools were not really doing history, in the sense of creating meaning from data. Similarly, many of the tools performed some kind of linguistic analysis, prompting many users to question how much data analysis can tell us about content and meaning. In reference to SEASR, one panelist complained that "whereas it may help determine frequency or clustering, it doesn’t tell me how or why. As I have indicated with other mining tools, this kind of tool can only take me so far, then I must consult other sources and methods to know how and why something happened in the past." Similarly, there seemed to be confusion about what the Favoured Traces tool was meant to do. One panelist felt compelled to point out that "it is really better suited to directing you to portions to study more closely than it is as a tool to actually do the studying." This of course is one of the main goals of the visualization. Similarly, there was also concern that the use of these tools strayed too far beyond the fundmental purpose of the humanities: "I could do with less jargon and more about how to use it. I am suspicious of the flow charts, perhaps that’s just me, but I thought the humanities are about matters that resist measure-and-manage control."

Needless to say, these important epistemological concerns are neither new nor exclusive to the digital humanities. What is worth noting, however, is that users generally identified perceived shortcomings of textual analysis or visualizations as problems with an individual tool, not with the methodology itself. On one hand, their comments may be an inadvertent conflation of a tool and the research methodology it facilitates. On the other hand, they may suggest that our non-technical users have a fundamental misunderstanding of what technology is truly capable of in terms of historical work. Perhaps they were led to believe that the tools were capable of much more than they are - and probably more than even the tools' creators would claim. For example, one respondent asked if Mark Davies's Time Magazine corpus (and interface to it) constitutes "a successful interpretive tool in itself or only a step along the way to understanding and interpretation." Not many tool builders would likely claim that a visually attractive word frequency diagram could possibly be considered a valuable interpretation in its own right. However, tool builders must be aware of this mindset. Addressing it directly will help users understand not only what the tools can actually do, but also what profitable applications of the tool might be in the course of their research.  
The difficulties surrounding data standardization became very apparent. When asked to evaluate a Shaping the West project from the Stanford Spatial History Project that produces a dynamic visualization of board members of U.S. railroad companies from 1872-1894, several respondents complained that they wanted to see their own data represented in a similar fashion. However, they didn’t ask about or comment on the level of difficulty to do that (which would be rather substantial). When presented with Many Eyes, an online tool from IBM that allows standardized data to be uploaded and viewed in a variety of formats (though nothing as visually dynamic as Shaping the West), users complained that they couldn’t get the data to display as they liked. Users were unclear about how to standardize the data in a useful way, even when they knew they needed to do that.

This observation underscores the need for example cases and perhaps a bit of hand-holding so that users can plainly see the all of the necessary preparatory steps, not just tutorials on using the tool itself after data has been properly prepared. Many Eyes, for example, simply assumes that one can and will figure out how to format the data to get the desired representation. For the kinds of researchers who built the tool, it may be obvious how data must be formatted, or seemingly intuitive to figure out. For historians, however, it seems that the tools are much more of a black box. Inability to represent the data in different ways was thought to be the fault of an uncooperative interface rather than a problem with how the data was uploaded. It seems that providing more education about manipulating data is essential to improve adoption of tools that depend on carefully formatted data. Recognizing that everyone has idiosyntractic work habits, we do not suggest that everyone must organize data in the same way. Rather we hope that tools will begin to play a more active role in educating users about the data formats most useful to the tool, and also in providing examples of the most common transpositional steps required to interface data with the tool.

Even if users were able to create visualizations, they were often unclear about what to do with the tool output. One respondent lamented that "most of these [tools] just produced fairly pretty looking or complicated displays that I couldn’t really figure out how to use. The things that I am looking for are ways to take structured data, that I might gather about events, organizations, people, etc. and create interactive maps, or visualizations of the links between people." This is obviously easier said than done. But this also suggests that because interaction is crucial, the user-interface cannot be thrown together after the fact. Furthermore, both the interface and documentation must err on the side of obvious rather than clever. For example, several visitors to Favored Traces commented that they found it utterly useless because you could not see the text itself, but only a representation of it. In fact, simply hovering over the 'visual text' reveals the actual text, and not much more work by the user can deliver a file of the entire edition. 
Lessons from Respondents  With the goal of making tools more easily adopted by the average historian, we have tried to distill the suggestions from our user feedback into a comparatively small set of considerations for tool builders. Most are not groundbreaking suggestions, but they underscore the real needs of interested users. Digital humanists should refrain from complaining about the limited acceptance of their methodologies when the tools and techniques that they have developed remain opaque or even unintelligible to an interested humanities audience.

Tools have generally neglected the typical humanities user in their design and documentation.   Builders of digital humanities tools, especially those that deal with technically more sophisticated techniques, like text-mining and visualization, could considerably increase their tools' visibility and speed of adoption with more attention to their user interface and clear instructions with example use cases. The intended audience of most tools, to the extent that a discernable one presents itself, seems to be technically sophisticated users who are already sold on the value and utility of the tool and who are willing to play around with the tool to get a sense of its possibilities. But as our panelists' interests suggests, the potential audience is far larger. The philosophy of "Don't Make Me Think" comes to mind as the kind of usability experiences the participants expected (Krug, 2000). Scholars approach scholarly software as software first and scholarship second. Any intellectual nuance that might be useful to the visitor must come after having met their expectations for web and software design. That means simply and clearly indicating what the tool is, how one uses it, and making it as easy as possible for one to get started and to see some initial results, even if only approximate, from the tool. While it is important to minimize the black-box problem by explaining how the tool works, it is equally important that such explanations don't crowd out a more basic explanation for new users.

*Provide concrete examples and explain the methodological value.*  Documentation needs to be non-technical in two ways. First, and most obviously, it must explain the basics of how to operate the tool. Similarly, though most tools do not do so now, it would be extremely helpful to provide examples about using the tool itself - that is, present specific examples of analysis across several disciplines. The cost outlay to create such content is not negligible, but the benefit would be disproportionately high. As one participant noted, "I think that the continued creation of these tools is really important, and think that they need to be explained, with some real-world examples, so that we can have a better sense whether the tool can do what we want." This also suggests that users might benefit from some explanation about the tool could help them do things that they didn't know they wanted to do. The second, and perhaps more crucial aspect of the documentation, will explain in general terms how the methodology of the tool can be useful. This would provide important motivation for the curious scholar who has come across the tool (or has been directed to it) but remains skeptical of the value of a new methodology. Documentation should explain, with examples, how the research methodology that the tool embraces can be useful and appeal to users across a variety of disciplines. Especially if tool builders typically see their role as making a methodology more accessible to scholars, they should include some justification and explanation of the methodology in their documentation. Even if methodlogical diffusion is not the principal goal of the project, explicit attention to it will only further the larger mission and adoption of the tool. 

*Be clear about the limitations of the tool and set reasonable expectations.*  Though it may appear obvious to the technically sophisticated humanist tool producer, tool introductions need to be clear that they neither function as substitutes for historical research nor attempt to produce historical knowledge. It perhaps cannot be overemphasized to the new user that the tools simply facilitate historical research by revealing trends or themes that might have otherwise gone unnoticed. To interpret what such trends or themes might mean remains the work of the historian. For the time being, then, tool builders might tone down the rhetoric about the interpretive power of the tools and how they can revolutionize research. Similarly, they should be encouraging users to think more deeply about the way tools create different views of, and interactions with, information which can then be interpreted and used as a means for developing and exploring historical arguments. Certainly, technically sophisticated users will have a better understanding of how a tool works, and will use the tools in more complex ways to facilitate their own analysis. But this should not be the only audience that developers and their tools try to engage with.

*Allocate more resources to user interface development.*  The user interface for many digital projects often seems developed as an after thought, thrown together after completing the core functionality. However, a truly good user interface requires significant investment in design and development that needs to be integrated into the project timeline and budget. It needs to be flexible to accommodate expanding tool features. Scholarly software designers should more consider the research on user-centered design approaches (e.g. Brown, 2004; Garett, 2002) and theories associated with user experience. Development should also include extensive testing, as many panelists were frustrated by bugs and crashes. Though some instability is unavoidable with prototype tools, scholars were almost resentful that they had invested time in a tool that was wasted out because of a critical failure--and thus lessened the likelihood they will return to the tool even after stability is improved. 

*No tool is an island.*  As digital tools become more easily accessible and more primary sources become available online, data standardization becomes even more crucial. To this end, tool builders might collaborate with data repositories and other tools to encourage compatibility between different formatting standards. This not to say that all humanists and repositories must adhere to the same standards or data formats. No single approach cannot possibly accommodate the myriad kinds of resources and institutions who are making data available. But there is a willing audience at hand, and some training about data standardization, since it's not exactly widespread in typical  humanities training, could offer tremendous boost to tool adoption. Our panelists were excited and inspired by the dynamic visualization of Shaping the West; their technological uncertainty hardly deterred them from digging into Many Eyes to produce unique views of their data. However, data roadblocks were fatal. For the former, users found that substituting their own standardized data was impossible; for the latter, users found it too difficult to standardize their data in an appropriate way. Similarly, tools need to be as interoperable as possible, especially in terms of how they can import and export data. "People already have databases," lamented one participant, "...it would be nice to have easy ways of accessing the data." The larger sentiment was not just about access, but sharing the data between tools.

##Epilogue
Broader goals for digital humanities tools  The participants in our survey and our panel discussion showed a great deal of enthusiasm for digital research tools and eagerness to engage with them. Although their interest is demonstrable, unfortunately so is the insufficient usability of many digital humanities tools. As our panelists indicated, concerns about the extent to which digital humanities tool developers consider what humanities scholars actually want to accomplish (Warwick, 2004) remain strong. 

From explicit statements alone, it appears that scholars are about equally split as to whether widespread adoption of digital tools should happen in their field. Perhaps this reflects optimist/pessimist points of view about technological change generally. But what resonates most strongly from our panel discusion is that virtually all of the participants reported at some point a glimmer of hope with respect to how digital tools might help them to research in new ways and re-conceptualize their work. Yet their frustrations over steep (or insurmountable) learning curves considerably dampened their hopes. At the same time, our panelists made clear that if interesting results could be produced in a short time, they would be inspired to use the tools even more. Perhaps such rough and ready use should be a more explicit aim of digital humanities tool development. With the first wave of digital humanities tools having produced excellent experimental and prototypical work, the fundamental barrier to wider adoption of digital tools seems to lie now in quality interfaces, accessible documentation, and expectations management.

Many tools now seem to downplay the importance of the user interface and documentation with the implicit rationale that if someone is really interested, they’ll figure out how to make the tool relevant to their own work. Our survey and discussion shows that this is often not the case. There are plenty of interested, curious, and technically capable humanities researchers that have little time and patience for trial and error with new methodologies when they are uncertain of their value. However, they remain receptive to the possibilites offered by the tools. So, when considering a user sympathetic to the promise of digital history tools, some leading by the nose is not only helpful, but necessary. 

Furthermore, though often seemingly outside the scope of a tool-building project, tools should not only document their functionality, but also should explicitly encourage scholars to approach their work in new ways. And in the midst of embracing new kinds of methodological challenges and cutting-edge tool development, tool designers must not forget the importance of a simple and clear user interface. It must not only make it easy to use the tool in productive ways, but also explain what the tool is for, provide examples of how it can be used, and give non-technical details about how it works in order to minimize the skepticism of black-box analysis. Such ease of use will hopefully bring increased integration of technology in humanities instruction, especially in terms of research methodologies and awareness of the importance of data standardization so that humanists are better able to communicate with archivists, librarians, and technologists who tirelessly facilitate data exchange (whether analog or digital).  Taken as a whole, the tenor of the responses suggests that digital humanities tool development projects ought to increase the scope of their imagined audience. The audience for early tools was, and in some ways needed to be, other technically sophisticated humanists. The potential audience has broadened considerably. Put another way, tool builders might consider both their tools and their target audience as more transitory than revolutionary. Keeping the cautiously optimistic user in mind would encourage a wider user base and facilitate the traction of digital humanities methodologies. Traditional humanists are willing to venture down the digital path, but they need to feel comfortable along the way.

Furthermore, the interested but not technically sophisticated user---who perhaps has not been a member of the existing tools' primary target audience to date---can help improve the tools in many ways. Some of the most verbose posts in the discussion were in response to tools that generated the most frustration, thus providing valuable feedback for developers about what went wrong and when. Additionally, a wider audience would reveal even more potential uses for the tool. As our panelists experimented, even while confused about what they were doing, they often tried to use the tools in ways seemingly not envisioned by the developers. Facilitating a way to easily capture such recommendations and foster user-communities would help improve the tool usability and lift some of the burden of documentation from developers and project teams. To help encourage wider use and facilitate these benefits, promoting tool awareness ought to become a more significant component of post-development work. Simply producing a tool is no longer sufficient.

An emphasis on cultivating a broader audience must be a concern not only for tool builders, but also for funders of such tools, who must ensure that tools adequately account for the time and expense of quality interfaces and documentation. Prioritizing a wider audience can help further adoption of tools in general, and thus further the acceptance of their use and development as comprising legitimate scholarly work. Even scholars who are disinclined to use such tools themselves ought to be able to understand what it is that other people are doing with them and how their sophisticated use might well constitute valuable scholarship in itself.

##Works Cited
Bradley 2008 Bradley, J. "Thinking about interpretation: Pliny and scholarship in the humanities." Literary and Linguistic Computing, 23.3 (2008): 263-279.
Brown 2006 Brown, Dan. Communicating Design: Developing Web Site Documentation for Design and Planning. New York: New Riders Press.  
Garrett, 2002 Garrett, Jesse James. The Elements of User Experience User-Centered Design for the Web. New York: New Riders Press.   
 Krug 2005 Krug, Steve. Don't Make Me Think! A Common Sense Approach to Web Usability, 2nd Edition. New York: New Riders Press, 2005. 
Leary 2005 Leary, Patrick. "Googling the Victorians," Journal of Victorian Culture 10.1 (Spring 2005): 72-86.   
Schreibman & Hanlon 2010 Schreibman, Susan & Hanlon, Ann M. "Determining Value for Digital Humanities Tools: Report on a Survey of Tool Developers." Digital Humanities Quarterly 4.2 2010. Retrieved from http://digitalhumanities.org/dhq/vol/4/2/000083/000083.html 
Shilton 2009 Shilton, Katie. "Supporting Digital Tools for Humanists: Investigating Tool Infrastructure. Final Report." May, 2009. Accessed at http://www.clir.org/pubs/archives/ShiltonToolsfinal.pdf
Summit 2006 Summit on Digital Tools for the Humanities, 2006. A Report on the Summit on Digital Tools. University of Virginia, September 2005. 
Warwick 2004 Warwick, C. "Print scholarship and digital resources." In Schreibman, S., Siemens, R., and Unsworth, J. (eds.), A Companion to Digital Humanities. Oxford: Blackwell Publishing, pp. 369–70. 
